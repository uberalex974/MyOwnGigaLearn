Initializing RocketSim version 2.1.1, created by ZealanL...
Loading arena meshes for soccar...
   > Loaded 483 verts and 880 tris, hash: 0xa160baf9
   > Loaded 483 verts and 880 tris, hash: 0x2811eee8
   > Loaded 80 verts and 126 tris, hash: 0xb81ac8b9
   > Loaded 80 verts and 126 tris, hash: 0x760358d3
   > Loaded 80 verts and 126 tris, hash: 0x73ae4940
   > Loaded 80 verts and 126 tris, hash: 0x918f4a4e
   > Loaded 18 verts and 16 tris, hash: 0x1f8ee550
   > Loaded 18 verts and 16 tris, hash: 0x255ba8c1
   > Loaded 483 verts and 880 tris, hash: 0x14b84668
   > Loaded 483 verts and 880 tris, hash: 0xec759ebf
   > Loaded 536 verts and 983 tris, hash: 0x94fb0d5c
   > Loaded 536 verts and 983 tris, hash: 0xdea07102
   > Loaded 536 verts and 983 tris, hash: 0xbd4fbea8
   > Loaded 536 verts and 983 tris, hash: 0x39a47f63
   > Loaded 18 verts and 16 tris, hash: 0x3d79d25d
   > Loaded 18 verts and 16 tris, hash: 0xd84c7a68
RocketSim::Init(): Finished loading arena collision meshes:
 > Soccar: 16
 > Hoops: 0
Finished initializing RocketSim in 0.021s!
Learner::Learner():
	Checkpoint Save/Load Dir: "C:/Giga/GigaLearnCPP/checkpoints"
	Using CUDA GPU device...
	Initializing RocketSim...
	Creating envs...
	Making PPO learner...
PPOLearner: Set learning rate to [3.000000e-04, 3.000000e-04]
Model parameter counts:
	"critic": 297,729
	"policy": 309,210
	"shared_head": 176,128
	[Total]: 783,067
Loading most recent checkpoint in "C:/Giga/GigaLearnCPP/checkpoints"...
 > No checkpoints found, starting new model.
PolicyVersionManager::LoadVersions():
 > Loaded 0 versions(s)
Initializing MetricSender...
 > Starting run with ID : "91wvi4yo"...
 > MetricSender initalized.
========================================
Learner::Start():
	Obs size: 167
	Action amount: 90
Press 'Q' to save and quit!
DEBUG: ExperienceBuffer data device: cuda:0
DEBUG: Experience batch state device: cuda:0, indices device: cuda:0








========================================
Average Step Reward: 0.7439
Policy Entropy: 0.7766


Collection Steps/Second: 96,374.7969
Consumption Steps/Second: 266,469.0938
Overall Steps/Second: 70,776.7344

Collection Time: 4.8026
 - Inference Time: 1.0555
 - Env Step Time: 2.5914
Consumption Time: 1.7370
 - GAE Time: 0.4859
 - PPO Learn Time: 0.9392
Collected Timesteps: 462,848
Total Timesteps: 462,848
Total Iterations: 1




PPOLearner: Set learning rate to [3.000000e-04, 3.000000e-04]








========================================
Average Step Reward: 0.3732
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 192,673
Consumption Steps/Second: 1,616,669.2500
Overall Steps/Second: 172,155.6719

Collection Time: 1.0098
 - Inference Time: 0.2689
 - Env Step Time: 0.3523
Consumption Time: 0.1203
 - GAE Time: 0.0741
 - PPO Learn Time: 0.0034
Collected Timesteps: 194,560
Total Timesteps: 657,408
Total Iterations: 2












========================================
Average Step Reward: 0.3952
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 186,619.1406
Consumption Steps/Second: 3,677,289
Overall Steps/Second: 177,605.8125

Collection Time: 1.4047
 - Inference Time: 0.3539
 - Env Step Time: 0.5091
Consumption Time: 0.0713
 - GAE Time: 0.0390
 - PPO Learn Time: 0.0031
Collected Timesteps: 262,144
Total Timesteps: 919,552
Total Iterations: 3




PPOLearner: Set learning rate to [3.000000e-04, 3.000000e-04]








========================================
Average Step Reward: 0.7478
Policy Entropy: 0.7767

Policy Update Magnitude: 0.0364
Critic Update Magnitude: 0.0135

Collection Steps/Second: 21,692.0957
Consumption Steps/Second: 4,698.8848
Overall Steps/Second: 3,862.2534

Collection Time: 0.2832
 - Inference Time: 0.0092
 - Env Step Time: 0.0156
Consumption Time: 1.3075
 - GAE Time: 0.3611
 - PPO Learn Time: 0.7532
Collected Timesteps: 6,144
Total Timesteps: 925,696
Total Iterations: 4












========================================
Average Step Reward: 0.5697
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 176,341.4062
Consumption Steps/Second: 1,562,177.7500
Overall Steps/Second: 158,454.7500

Collection Time: 0.8478
 - Inference Time: 0.2256
 - Env Step Time: 0.2891
Consumption Time: 0.0957
 - GAE Time: 0.0635
 - PPO Learn Time: 0.0030
Collected Timesteps: 149,504
Total Timesteps: 1,075,200
Total Iterations: 5




PPOLearner: Set learning rate to [2.999999e-04, 2.999999e-04]








========================================
Average Step Reward: 0.4838
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 158,369.8750
Consumption Steps/Second: 1,722,436.3750
Overall Steps/Second: 145,034.6250

Collection Time: 0.8664
 - Inference Time: 0.2017
 - Env Step Time: 0.3414
Consumption Time: 0.0797
 - GAE Time: 0.0546
 - PPO Learn Time: 0.0030
Collected Timesteps: 137,216
Total Timesteps: 1,212,416
Total Iterations: 6












========================================
Average Step Reward: 0.6865
Policy Entropy: 0.7758

Policy Update Magnitude: 0.0250
Critic Update Magnitude: 0.0141

Collection Steps/Second: 146,388.5469
Consumption Steps/Second: 140,730.4375
Overall Steps/Second: 71,751.8672

Collection Time: 1.2032
 - Inference Time: 0.2169
 - Env Step Time: 0.4785
Consumption Time: 1.2515
 - GAE Time: 0.3433
 - PPO Learn Time: 0.7354
Collected Timesteps: 176,128
Total Timesteps: 1,388,544
Total Iterations: 7




PPOLearner: Set learning rate to [2.999998e-04, 2.999998e-04]








========================================
Average Step Reward: 0.5954
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 69,388.9141
Consumption Steps/Second: 1,065,240.5000
Overall Steps/Second: 65,145.3984

Collection Time: 1.6086
 - Inference Time: 0.8484
 - Env Step Time: 0.3614
Consumption Time: 0.1048
 - GAE Time: 0.0693
 - PPO Learn Time: 0.0033
Collected Timesteps: 111,616
Total Timesteps: 1,500,160
Total Iterations: 8












========================================
Average Step Reward: 0.6458
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 163,079.9375
Consumption Steps/Second: 1,347,060.6250
Overall Steps/Second: 145,468.9531

Collection Time: 0.9042
 - Inference Time: 0.2023
 - Env Step Time: 0.3822
Consumption Time: 0.1095
 - GAE Time: 0.0811
 - PPO Learn Time: 0.0037
Collected Timesteps: 147,456
Total Timesteps: 1,647,616
Total Iterations: 9




PPOLearner: Set learning rate to [2.999998e-04, 2.999998e-04]








========================================
Average Step Reward: 0.6416
Policy Entropy: 0.7711

Policy Update Magnitude: 0.0136
Critic Update Magnitude: 0.0096

Collection Steps/Second: 129,882.4062
Consumption Steps/Second: 97,307.5391
Overall Steps/Second: 55,629.8281

Collection Time: 0.7096
 - Inference Time: 0.1326
 - Env Step Time: 0.2202
Consumption Time: 0.9471
 - GAE Time: 0.3131
 - PPO Learn Time: 0.4905
Collected Timesteps: 92,160
Total Timesteps: 1,739,776
Total Iterations: 10




PPOLearner: Set learning rate to [2.999998e-04, 2.999998e-04]








========================================
Average Step Reward: 0.8201
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 66,094.6172
Consumption Steps/Second: 771,268.2500
Overall Steps/Second: 60,877.6445

Collection Time: 1.1155
 - Inference Time: 0.5975
 - Env Step Time: 0.2406
Consumption Time: 0.0956
 - GAE Time: 0.0543
 - PPO Learn Time: 0.0029
Collected Timesteps: 73,728
Total Timesteps: 1,813,504
Total Iterations: 11












========================================
Average Step Reward: 0.5741
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 185,685.5625
Consumption Steps/Second: 1,121,766
Overall Steps/Second: 159,314.3125

Collection Time: 0.5956
 - Inference Time: 0.1426
 - Env Step Time: 0.2099
Consumption Time: 0.0986
 - GAE Time: 0.0721
 - PPO Learn Time: 0.0038
Collected Timesteps: 110,592
Total Timesteps: 1,924,096
Total Iterations: 12




PPOLearner: Set learning rate to [2.999997e-04, 2.999997e-04]








========================================
Average Step Reward: 93.6586
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 67,815.1406
Consumption Steps/Second: 380,465.0625
Overall Steps/Second: 57,556.1719

Collection Time: 1.5100
 - Inference Time: 0.8168
 - Env Step Time: 0.3259
Consumption Time: 0.2691
 - GAE Time: 0.2034
 - PPO Learn Time: 0.0036
Collected Timesteps: 102,400
Total Timesteps: 2,026,496
Total Iterations: 13




PPOLearner: Set learning rate to [2.999997e-04, 2.999997e-04]








========================================
Average Step Reward: 0.5794
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 197,070.8906
Consumption Steps/Second: 1,196,354.8750
Overall Steps/Second: 169,199.3438

Collection Time: 0.5924
 - Inference Time: 0.1512
 - Env Step Time: 0.2061
Consumption Time: 0.0976
 - GAE Time: 0.0711
 - PPO Learn Time: 0.0032
Collected Timesteps: 116,736
Total Timesteps: 2,143,232
Total Iterations: 14




PPOLearner: Set learning rate to [2.999996e-04, 2.999996e-04]








========================================
Average Step Reward: 0.6757
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 180,322.4531
Consumption Steps/Second: 671,363
Overall Steps/Second: 142,143.8281

Collection Time: 0.3180
 - Inference Time: 0.0721
 - Env Step Time: 0.1136
Consumption Time: 0.0854
 - GAE Time: 0.0610
 - PPO Learn Time: 0.0030
Collected Timesteps: 57,344
Total Timesteps: 2,200,576
Total Iterations: 15












========================================
Average Step Reward: 0.5991
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 201,572.9062
Consumption Steps/Second: 1,013,474.6250
Overall Steps/Second: 168,132.5312

Collection Time: 0.4877
 - Inference Time: 0.1194
 - Env Step Time: 0.1798
Consumption Time: 0.0970
 - GAE Time: 0.0712
 - PPO Learn Time: 0.0034
Collected Timesteps: 98,304
Total Timesteps: 2,298,880
Total Iterations: 16




PPOLearner: Set learning rate to [2.999996e-04, 2.999996e-04]








========================================
Average Step Reward: 345,927.4062
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 181,182.0938
Consumption Steps/Second: 958,743.2500
Overall Steps/Second: 152,384.6250

Collection Time: 0.4182
 - Inference Time: 0.0942
 - Env Step Time: 0.1462
Consumption Time: 0.0790
 - GAE Time: 0.0523
 - PPO Learn Time: 0.0033
Collected Timesteps: 75,776
Total Timesteps: 2,374,656
Total Iterations: 17




PPOLearner: Set learning rate to [2.999995e-04, 2.999995e-04]








========================================
Average Step Reward: 0.6900
Policy Entropy: 0.7786

Policy Update Magnitude: 0.0133
Critic Update Magnitude: 0.0098

Collection Steps/Second: 159,353.2656
Consumption Steps/Second: 141,475.6406
Overall Steps/Second: 74,941.6250

Collection Time: 0.7197
 - Inference Time: 0.1297
 - Env Step Time: 0.2444
Consumption Time: 0.8107
 - GAE Time: 0.2281
 - PPO Learn Time: 0.4416
Collected Timesteps: 114,688
Total Timesteps: 2,489,344
Total Iterations: 18




PPOLearner: Set learning rate to [2.999995e-04, 2.999995e-04]








========================================
Average Step Reward: 0.6027
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 170,233.4688
Consumption Steps/Second: 1,231,685.7500
Overall Steps/Second: 149,562.2188

Collection Time: 0.6617
 - Inference Time: 0.1451
 - Env Step Time: 0.2537
Consumption Time: 0.0915
 - GAE Time: 0.0615
 - PPO Learn Time: 0.0035
Collected Timesteps: 112,640
Total Timesteps: 2,601,984
Total Iterations: 19




PPOLearner: Set learning rate to [2.999995e-04, 2.999995e-04]








========================================
Average Step Reward: 0.6294
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 164,738.4844
Consumption Steps/Second: 391,958.4375
Overall Steps/Second: 115,988.8594

Collection Time: 0.1989
 - Inference Time: 0.0358
 - Env Step Time: 0.0710
Consumption Time: 0.0836
 - GAE Time: 0.0598
 - PPO Learn Time: 0.0030
Collected Timesteps: 32,768
Total Timesteps: 2,634,752
Total Iterations: 20




PPOLearner: Set learning rate to [2.999994e-04, 2.999994e-04]








========================================
Average Step Reward: 0.6351
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 193,424.9844
Consumption Steps/Second: 829,059.6875
Overall Steps/Second: 156,834.4844

Collection Time: 0.4023
 - Inference Time: 0.0865
 - Env Step Time: 0.1456
Consumption Time: 0.0939
 - GAE Time: 0.0669
 - PPO Learn Time: 0.0031
Collected Timesteps: 77,824
Total Timesteps: 2,712,576
Total Iterations: 21




PPOLearner: Set learning rate to [2.999994e-04, 2.999994e-04]








========================================
Average Step Reward: 0.6531
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 187,728.6250
Consumption Steps/Second: 765,323.0625
Overall Steps/Second: 150,750.5312

Collection Time: 0.3709
 - Inference Time: 0.0761
 - Env Step Time: 0.1376
Consumption Time: 0.0910
 - GAE Time: 0.0637
 - PPO Learn Time: 0.0034
Collected Timesteps: 69,632
Total Timesteps: 2,782,208
Total Iterations: 22




PPOLearner: Set learning rate to [2.999994e-04, 2.999994e-04]








========================================
Average Step Reward: 0.6179
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 199,424.0781
Consumption Steps/Second: 1,278,437.3750
Overall Steps/Second: 172,513.5938

Collection Time: 0.6264
 - Inference Time: 0.1380
 - Env Step Time: 0.2458
Consumption Time: 0.0977
 - GAE Time: 0.0651
 - PPO Learn Time: 0.0029
Collected Timesteps: 124,928
Total Timesteps: 2,907,136
Total Iterations: 23




PPOLearner: Set learning rate to [2.999993e-04, 2.999993e-04]








========================================
Average Step Reward: 0.6892
Policy Entropy: 0.7753

Policy Update Magnitude: 0.0062
Critic Update Magnitude: 0.0050

Collection Steps/Second: 142,643.2656
Consumption Steps/Second: 90,341.0547
Overall Steps/Second: 55,310.7734

Collection Time: 0.3159
 - Inference Time: 0.0588
 - Env Step Time: 0.0989
Consumption Time: 0.4987
 - GAE Time: 0.1902
 - PPO Learn Time: 0.2304
Collected Timesteps: 45,056
Total Timesteps: 2,952,192
Total Iterations: 24












========================================
Average Step Reward: 0.5483
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 160,768.4844
Consumption Steps/Second: 1,167,450.5000
Overall Steps/Second: 141,308.9688

Collection Time: 0.6369
 - Inference Time: 0.1355
 - Env Step Time: 0.2515
Consumption Time: 0.0877
 - GAE Time: 0.0617
 - PPO Learn Time: 0.0031
Collected Timesteps: 102,400
Total Timesteps: 3,054,592
Total Iterations: 25




PPOLearner: Set learning rate to [2.999993e-04, 2.999993e-04]








========================================
Average Step Reward: 0.6379
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 150,333.0938
Consumption Steps/Second: 357,566.4688
Overall Steps/Second: 105,836.0391

Collection Time: 0.2180
 - Inference Time: 0.0506
 - Env Step Time: 0.0734
Consumption Time: 0.0916
 - GAE Time: 0.0677
 - PPO Learn Time: 0.0031
Collected Timesteps: 32,768
Total Timesteps: 3,087,360
Total Iterations: 26




PPOLearner: Set learning rate to [2.999992e-04, 2.999992e-04]








========================================
Average Step Reward: 0.6883
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 187,857.7656
Consumption Steps/Second: 811,867.7500
Overall Steps/Second: 152,557.5312

Collection Time: 0.4034
 - Inference Time: 0.1060
 - Env Step Time: 0.1336
Consumption Time: 0.0933
 - GAE Time: 0.0693
 - PPO Learn Time: 0.0027
Collected Timesteps: 75,776
Total Timesteps: 3,163,136
Total Iterations: 27




PPOLearner: Set learning rate to [2.999992e-04, 2.999992e-04]








========================================
Average Step Reward: 0.6361
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 183,933.7188
Consumption Steps/Second: 796,525.5625
Overall Steps/Second: 149,427.8438

Collection Time: 0.3674
 - Inference Time: 0.0914
 - Env Step Time: 0.1203
Consumption Time: 0.0848
 - GAE Time: 0.0621
 - PPO Learn Time: 0.0029
Collected Timesteps: 67,584
Total Timesteps: 3,230,720
Total Iterations: 28




PPOLearner: Set learning rate to [2.999992e-04, 2.999992e-04]








========================================
Average Step Reward: 0.6496
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 188,909.5938
Consumption Steps/Second: 1,238,966.3750
Overall Steps/Second: 163,916.6406

Collection Time: 0.5854
 - Inference Time: 0.1467
 - Env Step Time: 0.2075
Consumption Time: 0.0893
 - GAE Time: 0.0653
 - PPO Learn Time: 0.0029
Collected Timesteps: 110,592
Total Timesteps: 3,341,312
Total Iterations: 29




PPOLearner: Set learning rate to [2.999991e-04, 2.999991e-04]








========================================
Average Step Reward: 0.6913
Policy Entropy: 0.7766

Policy Update Magnitude: 0.0059
Critic Update Magnitude: 0.0050

Collection Steps/Second: 172,355.1875
Consumption Steps/Second: 137,949.3281
Overall Steps/Second: 76,622.4219

Collection Time: 0.4278
 - Inference Time: 0.0957
 - Env Step Time: 0.1426
Consumption Time: 0.5345
 - GAE Time: 0.1873
 - PPO Learn Time: 0.2541
Collected Timesteps: 73,728
Total Timesteps: 3,415,040
Total Iterations: 30




PPOLearner: Set learning rate to [2.999991e-04, 2.999991e-04]








========================================
Average Step Reward: 0.5901
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 162,852.6094
Consumption Steps/Second: 872,686.8125
Overall Steps/Second: 137,241.8281

Collection Time: 0.6162
 - Inference Time: 0.1630
 - Env Step Time: 0.2337
Consumption Time: 0.1150
 - GAE Time: 0.0665
 - PPO Learn Time: 0.0040
Collected Timesteps: 100,352
Total Timesteps: 3,515,392
Total Iterations: 31




PPOLearner: Set learning rate to [2.999990e-04, 2.999990e-04]
Running skill matches (simTime=30)...
 > Forcing continuation (0/8)








========================================
Average Step Reward: 0.6603
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 167,187.2344
Consumption Steps/Second: 337,147.6875
Overall Steps/Second: 111,764.6016

Collection Time: 0.2205
 - Inference Time: 0.0492
 - Env Step Time: 0.0688
Consumption Time: 0.1093
 - GAE Time: 0.0715
 - PPO Learn Time: 0.0027
Collected Timesteps: 36,864
Total Timesteps: 3,552,256
Total Iterations: 32




PPOLearner: Set learning rate to [2.999990e-04, 2.999990e-04]








========================================
Average Step Reward: 0.6579
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 77,505.3125
Consumption Steps/Second: 783,282.3750
Overall Steps/Second: 70,526.7344

Collection Time: 0.8984
 - Inference Time: 0.4930
 - Env Step Time: 0.1815
Consumption Time: 0.0889
 - GAE Time: 0.0644
 - PPO Learn Time: 0.0030
Collected Timesteps: 69,632
Total Timesteps: 3,621,888
Total Iterations: 33




PPOLearner: Set learning rate to [2.999989e-04, 2.999989e-04]








========================================
Average Step Reward: 0.6570
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 197,118.5312
Consumption Steps/Second: 887,834
Overall Steps/Second: 161,305.2500

Collection Time: 0.4987
 - Inference Time: 0.1126
 - Env Step Time: 0.1960
Consumption Time: 0.1107
 - GAE Time: 0.0851
 - PPO Learn Time: 0.0036
Collected Timesteps: 98,304
Total Timesteps: 3,720,192
Total Iterations: 34




PPOLearner: Set learning rate to [2.999989e-04, 2.999989e-04]








========================================
Average Step Reward: 0.7539
Policy Entropy: 0.7798

Policy Update Magnitude: 0.0062
Critic Update Magnitude: 0.0051

Collection Steps/Second: 179,002.0312
Consumption Steps/Second: 180,336.1406
Overall Steps/Second: 89,833.3047

Collection Time: 0.4920
 - Inference Time: 0.1068
 - Env Step Time: 0.1732
Consumption Time: 0.4883
 - GAE Time: 0.1852
 - PPO Learn Time: 0.2197
Collected Timesteps: 88,064
Total Timesteps: 3,808,256
Total Iterations: 35




PPOLearner: Set learning rate to [2.999989e-04, 2.999989e-04]








========================================
Average Step Reward: 0.6382
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 193,562.0469
Consumption Steps/Second: 851,764.8125
Overall Steps/Second: 157,720.3750

Collection Time: 0.5396
 - Inference Time: 0.1314
 - Env Step Time: 0.2048
Consumption Time: 0.1226
 - GAE Time: 0.0808
 - PPO Learn Time: 0.0030
Collected Timesteps: 104,448
Total Timesteps: 3,912,704
Total Iterations: 36




PPOLearner: Set learning rate to [2.999988e-04, 2.999988e-04]








========================================
Average Step Reward: 0.5841
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 182,765.6719
Consumption Steps/Second: 447,733.6875
Overall Steps/Second: 129,786.5625

Collection Time: 0.2353
 - Inference Time: 0.0469
 - Env Step Time: 0.0879
Consumption Time: 0.0961
 - GAE Time: 0.0715
 - PPO Learn Time: 0.0029
Collected Timesteps: 43,008
Total Timesteps: 3,955,712
Total Iterations: 37




PPOLearner: Set learning rate to [2.999987e-04, 2.999987e-04]








========================================
Average Step Reward: 0.6342
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 182,703.9219
Consumption Steps/Second: 689,952.9375
Overall Steps/Second: 144,452.0938

Collection Time: 0.3027
 - Inference Time: 0.0634
 - Env Step Time: 0.1123
Consumption Time: 0.0801
 - GAE Time: 0.0546
 - PPO Learn Time: 0.0027
Collected Timesteps: 55,296
Total Timesteps: 4,011,008
Total Iterations: 38




PPOLearner: Set learning rate to [2.999987e-04, 2.999987e-04]








========================================
Average Step Reward: 0.6198
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 180,453.6719
Consumption Steps/Second: 729,671.3125
Overall Steps/Second: 144,674.4844

Collection Time: 0.3405
 - Inference Time: 0.0783
 - Env Step Time: 0.1193
Consumption Time: 0.0842
 - GAE Time: 0.0599
 - PPO Learn Time: 0.0031
Collected Timesteps: 61,440
Total Timesteps: 4,072,448
Total Iterations: 39




PPOLearner: Set learning rate to [2.999986e-04, 2.999986e-04]








========================================
Average Step Reward: 0.6593
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 192,896.2031
Consumption Steps/Second: 929,937.8750
Overall Steps/Second: 159,757.7812

Collection Time: 0.4565
 - Inference Time: 0.1012
 - Env Step Time: 0.1760
Consumption Time: 0.0947
 - GAE Time: 0.0708
 - PPO Learn Time: 0.0029
Collected Timesteps: 88,064
Total Timesteps: 4,160,512
Total Iterations: 40




PPOLearner: Set learning rate to [2.999986e-04, 2.999986e-04]








========================================
Average Step Reward: 0.7203
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 76,331.9688
Consumption Steps/Second: 440,229.8750
Overall Steps/Second: 65,052.4453

Collection Time: 0.7244
 - Inference Time: 0.3909
 - Env Step Time: 0.1462
Consumption Time: 0.1256
 - GAE Time: 0.0922
 - PPO Learn Time: 0.0032
Collected Timesteps: 55,296
Total Timesteps: 4,215,808
Total Iterations: 41




PPOLearner: Set learning rate to [2.999986e-04, 2.999986e-04]








========================================
Average Step Reward: 0.6531
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 194,368.5312
Consumption Steps/Second: 1,050,490.2500
Overall Steps/Second: 164,020.4062

Collection Time: 0.4952
 - Inference Time: 0.1079
 - Env Step Time: 0.1953
Consumption Time: 0.0916
 - GAE Time: 0.0667
 - PPO Learn Time: 0.0028
Collected Timesteps: 96,256
Total Timesteps: 4,312,064
Total Iterations: 42




PPOLearner: Set learning rate to [2.999985e-04, 2.999985e-04]








========================================
Average Step Reward: 0.6841
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 174,363.4688
Consumption Steps/Second: 317,397.1250
Overall Steps/Second: 112,539.4453

Collection Time: 0.2232
 - Inference Time: 0.0472
 - Env Step Time: 0.0772
Consumption Time: 0.1226
 - GAE Time: 0.0937
 - PPO Learn Time: 0.0032
Collected Timesteps: 38,912
Total Timesteps: 4,350,976
Total Iterations: 43












========================================
Average Step Reward: 0.6892
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 179,614.9375
Consumption Steps/Second: 508,986.1250
Overall Steps/Second: 132,764.1094

Collection Time: 0.3079
 - Inference Time: 0.0691
 - Env Step Time: 0.1035
Consumption Time: 0.1086
 - GAE Time: 0.0833
 - PPO Learn Time: 0.0032
Collected Timesteps: 55,296
Total Timesteps: 4,406,272
Total Iterations: 44




PPOLearner: Set learning rate to [2.999984e-04, 2.999984e-04]








========================================
Average Step Reward: 0.6642
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 182,603.6406
Consumption Steps/Second: 733,845.0625
Overall Steps/Second: 146,219.6094

Collection Time: 0.3925
 - Inference Time: 0.1004
 - Env Step Time: 0.1302
Consumption Time: 0.0977
 - GAE Time: 0.0729
 - PPO Learn Time: 0.0032
Collected Timesteps: 71,680
Total Timesteps: 4,477,952
Total Iterations: 45




PPOLearner: Set learning rate to [2.999984e-04, 2.999984e-04]








========================================
Average Step Reward: 0.6590
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 183,977.4688
Consumption Steps/Second: 1,036,646
Overall Steps/Second: 156,247.6094

Collection Time: 0.5566
 - Inference Time: 0.1475
 - Env Step Time: 0.1951
Consumption Time: 0.0988
 - GAE Time: 0.0748
 - PPO Learn Time: 0.0032
Collected Timesteps: 102,400
Total Timesteps: 4,580,352
Total Iterations: 46




PPOLearner: Set learning rate to [2.999983e-04, 2.999983e-04]








========================================
Average Step Reward: 0.6454
Policy Entropy: 0.7745

Policy Update Magnitude: 0.0060
Critic Update Magnitude: 0.0051

Collection Steps/Second: 67,680.0703
Consumption Steps/Second: 124,290.4141
Overall Steps/Second: 43,819.1562

Collection Time: 0.7262
 - Inference Time: 0.3713
 - Env Step Time: 0.1483
Consumption Time: 0.3955
 - GAE Time: 0.0770
 - PPO Learn Time: 0.2550
Collected Timesteps: 49,152
Total Timesteps: 4,629,504
Total Iterations: 47




PPOLearner: Set learning rate to [2.999983e-04, 2.999983e-04]








========================================
Average Step Reward: 1,465.7433
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 68,765.4062
Consumption Steps/Second: 509,839.5000
Overall Steps/Second: 60,592.8516

Collection Time: 1.1466
 - Inference Time: 0.6816
 - Env Step Time: 0.2243
Consumption Time: 0.1547
 - GAE Time: 0.1069
 - PPO Learn Time: 0.0030
Collected Timesteps: 78,848
Total Timesteps: 4,708,352
Total Iterations: 48




PPOLearner: Set learning rate to [2.999982e-04, 2.999982e-04]








========================================
Average Step Reward: 0.7240
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 182,396.0312
Consumption Steps/Second: 672,137.7500
Overall Steps/Second: 143,464.5000

Collection Time: 0.3930
 - Inference Time: 0.0961
 - Env Step Time: 0.1334
Consumption Time: 0.1066
 - GAE Time: 0.0799
 - PPO Learn Time: 0.0030
Collected Timesteps: 71,680
Total Timesteps: 4,780,032
Total Iterations: 49




PPOLearner: Set learning rate to [2.999982e-04, 2.999982e-04]








========================================
Average Step Reward: 0.7281
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 78,697.5781
Consumption Steps/Second: 908,548.6875
Overall Steps/Second: 72,424.2656

Collection Time: 1.4053
 - Inference Time: 0.7948
 - Env Step Time: 0.2758
Consumption Time: 0.1217
 - GAE Time: 0.0984
 - PPO Learn Time: 0.0035
Collected Timesteps: 110,592
Total Timesteps: 4,890,624
Total Iterations: 50




PPOLearner: Set learning rate to [2.999981e-04, 2.999981e-04]








========================================
Average Step Reward: 0.7357
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 128,115.8359
Consumption Steps/Second: 130,139.9844
Overall Steps/Second: 64,559.9883

Collection Time: 0.0959
 - Inference Time: 0.0179
 - Env Step Time: 0.0243
Consumption Time: 0.0944
 - GAE Time: 0.0715
 - PPO Learn Time: 0.0027
Collected Timesteps: 12,288
Total Timesteps: 4,902,912
Total Iterations: 51












========================================
Average Step Reward: 0.6452
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 196,816.2344
Consumption Steps/Second: 1,028,770.3125
Overall Steps/Second: 165,209.6250

Collection Time: 0.4787
 - Inference Time: 0.1202
 - Env Step Time: 0.1719
Consumption Time: 0.0916
 - GAE Time: 0.0654
 - PPO Learn Time: 0.0028
Collected Timesteps: 94,208
Total Timesteps: 4,997,120
Total Iterations: 52




PPOLearner: Set learning rate to [2.999980e-04, 2.999980e-04]








========================================
Average Step Reward: 0.7807
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 165,750.9375
Consumption Steps/Second: 408,036.7188
Overall Steps/Second: 117,870.2031

Collection Time: 0.1977
 - Inference Time: 0.0361
 - Env Step Time: 0.0734
Consumption Time: 0.0803
 - GAE Time: 0.0566
 - PPO Learn Time: 0.0030
Collected Timesteps: 32,768
Total Timesteps: 5,029,888
Total Iterations: 53




PPOLearner: Set learning rate to [2.999979e-04, 2.999979e-04]








========================================
Average Step Reward: 0.7304
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 181,082.2031
Consumption Steps/Second: 551,916.8750
Overall Steps/Second: 136,347.1250

Collection Time: 0.2941
 - Inference Time: 0.0632
 - Env Step Time: 0.1069
Consumption Time: 0.0965
 - GAE Time: 0.0733
 - PPO Learn Time: 0.0033
Collected Timesteps: 53,248
Total Timesteps: 5,083,136
Total Iterations: 54




PPOLearner: Set learning rate to [2.999979e-04, 2.999979e-04]








========================================
Average Step Reward: 0.6396
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 188,296.7656
Consumption Steps/Second: 713,812.0625
Overall Steps/Second: 148,993.6719

Collection Time: 0.4242
 - Inference Time: 0.0900
 - Env Step Time: 0.1556
Consumption Time: 0.1119
 - GAE Time: 0.0882
 - PPO Learn Time: 0.0030
Collected Timesteps: 79,872
Total Timesteps: 5,163,008
Total Iterations: 55




PPOLearner: Set learning rate to [2.999978e-04, 2.999978e-04]








========================================
Average Step Reward: 0.6218
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 183,281.3281
Consumption Steps/Second: 840,997.6250
Overall Steps/Second: 150,485.5312

Collection Time: 0.3687
 - Inference Time: 0.0761
 - Env Step Time: 0.1470
Consumption Time: 0.0804
 - GAE Time: 0.0567
 - PPO Learn Time: 0.0031
Collected Timesteps: 67,584
Total Timesteps: 5,230,592
Total Iterations: 56




PPOLearner: Set learning rate to [2.999978e-04, 2.999978e-04]








========================================
Average Step Reward: 0.5286
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 167,158.7031
Consumption Steps/Second: 792,282.4375
Overall Steps/Second: 138,035.4688

Collection Time: 0.3676
 - Inference Time: 0.0826
 - Env Step Time: 0.1415
Consumption Time: 0.0775
 - GAE Time: 0.0513
 - PPO Learn Time: 0.0030
Collected Timesteps: 61,440
Total Timesteps: 5,292,032
Total Iterations: 57




PPOLearner: Set learning rate to [2.999977e-04, 2.999977e-04]








========================================
Average Step Reward: 0.6547
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 180,442.5156
Consumption Steps/Second: 695,704.3750
Overall Steps/Second: 143,280.3594

Collection Time: 0.2724
 - Inference Time: 0.0560
 - Env Step Time: 0.1029
Consumption Time: 0.0707
 - GAE Time: 0.0445
 - PPO Learn Time: 0.0028
Collected Timesteps: 49,152
Total Timesteps: 5,341,184
Total Iterations: 58




PPOLearner: Set learning rate to [2.999977e-04, 2.999977e-04]








========================================
Average Step Reward: 0.7082
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 151,267.7344
Consumption Steps/Second: 260,488.5469
Overall Steps/Second: 95,696.2031

Collection Time: 0.1625
 - Inference Time: 0.0273
 - Env Step Time: 0.0598
Consumption Time: 0.0943
 - GAE Time: 0.0696
 - PPO Learn Time: 0.0026
Collected Timesteps: 24,576
Total Timesteps: 5,365,760
Total Iterations: 59




PPOLearner: Set learning rate to [2.999977e-04, 2.999977e-04]








========================================
Average Step Reward: 0.6286
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 192,248.6406
Consumption Steps/Second: 1,007,583.1250
Overall Steps/Second: 161,444.7031

Collection Time: 0.4794
 - Inference Time: 0.1043
 - Env Step Time: 0.1817
Consumption Time: 0.0915
 - GAE Time: 0.0640
 - PPO Learn Time: 0.0030
Collected Timesteps: 92,160
Total Timesteps: 5,457,920
Total Iterations: 60




PPOLearner: Set learning rate to [2.999976e-04, 2.999976e-04]








========================================
Average Step Reward: 0.6726
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 163,934.9375
Consumption Steps/Second: 398,608.5938
Overall Steps/Second: 116,161.4531

Collection Time: 0.2249
 - Inference Time: 0.0460
 - Env Step Time: 0.0802
Consumption Time: 0.0925
 - GAE Time: 0.0674
 - PPO Learn Time: 0.0027
Collected Timesteps: 36,864
Total Timesteps: 5,494,784
Total Iterations: 61




PPOLearner: Set learning rate to [2.999975e-04, 2.999975e-04]








========================================
Average Step Reward: -9,223,372,036,854,775,808
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 187,142.2500
Consumption Steps/Second: 641,598.2500
Overall Steps/Second: 144,882.6719

Collection Time: 0.3064
 - Inference Time: 0.0630
 - Env Step Time: 0.1177
Consumption Time: 0.0894
 - GAE Time: 0.0665
 - PPO Learn Time: 0.0028
Collected Timesteps: 57,344
Total Timesteps: 5,552,128
Total Iterations: 62




PPOLearner: Set learning rate to [2.999975e-04, 2.999975e-04]








========================================
Average Step Reward: 0.5790
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 177,144.9844
Consumption Steps/Second: 614,330.9375
Overall Steps/Second: 137,497.0938

Collection Time: 0.2890
 - Inference Time: 0.0711
 - Env Step Time: 0.0946
Consumption Time: 0.0833
 - GAE Time: 0.0583
 - PPO Learn Time: 0.0033
Collected Timesteps: 51,200
Total Timesteps: 5,603,328
Total Iterations: 63




PPOLearner: Set learning rate to [2.999975e-04, 2.999975e-04]
Running skill matches (simTime=30)...
 >  > 2v2 = 0 (+2.5)
 > Forcing continuation (1/8)








========================================
Average Step Reward: 0.5973
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 190,607.2344
Consumption Steps/Second: 806,096.7500
Overall Steps/Second: 154,155.9844

Collection Time: 0.3976
 - Inference Time: 0.0861
 - Env Step Time: 0.1514
Consumption Time: 0.0940
 - GAE Time: 0.0673
 - PPO Learn Time: 0.0031
Collected Timesteps: 75,776
Total Timesteps: 5,679,104
Total Iterations: 64




PPOLearner: Set learning rate to [2.999974e-04, 2.999974e-04]








========================================
Average Step Reward: 0.6615
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 176,790.7031
Consumption Steps/Second: 890,699.2500
Overall Steps/Second: 147,511.7812

Collection Time: 0.4750
 - Inference Time: 0.1292
 - Env Step Time: 0.1672
Consumption Time: 0.0943
 - GAE Time: 0.0703
 - PPO Learn Time: 0.0031
Collected Timesteps: 83,968
Total Timesteps: 5,763,072
Total Iterations: 65




PPOLearner: Set learning rate to [2.999973e-04, 2.999973e-04]








========================================
Average Step Reward: 0.6643
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 166,120.2500
Consumption Steps/Second: 398,724.0625
Overall Steps/Second: 117,264.4141

Collection Time: 0.3945
 - Inference Time: 0.0920
 - Env Step Time: 0.1425
Consumption Time: 0.1644
 - GAE Time: 0.1101
 - PPO Learn Time: 0.0031
Collected Timesteps: 65,536
Total Timesteps: 5,828,608
Total Iterations: 66




PPOLearner: Set learning rate to [2.999972e-04, 2.999972e-04]








========================================
Average Step Reward: 0.6599
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 188,997.1875
Consumption Steps/Second: 810,533.4375
Overall Steps/Second: 153,260.4688

Collection Time: 0.4876
 - Inference Time: 0.1373
 - Env Step Time: 0.1610
Consumption Time: 0.1137
 - GAE Time: 0.0724
 - PPO Learn Time: 0.0030
Collected Timesteps: 92,160
Total Timesteps: 5,920,768
Total Iterations: 67




PPOLearner: Set learning rate to [2.999971e-04, 2.999971e-04]








========================================
Average Step Reward: 0.6955
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 167,650.6562
Consumption Steps/Second: 412,264.5312
Overall Steps/Second: 119,183.6641

Collection Time: 0.2321
 - Inference Time: 0.0471
 - Env Step Time: 0.0844
Consumption Time: 0.0944
 - GAE Time: 0.0628
 - PPO Learn Time: 0.0029
Collected Timesteps: 38,912
Total Timesteps: 5,959,680
Total Iterations: 68




PPOLearner: Set learning rate to [2.999971e-04, 2.999971e-04]








========================================
Average Step Reward: 0.6401
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 165,331.7344
Consumption Steps/Second: 440,984.3125
Overall Steps/Second: 120,248.6797

Collection Time: 0.3097
 - Inference Time: 0.0739
 - Env Step Time: 0.1005
Consumption Time: 0.1161
 - GAE Time: 0.0872
 - PPO Learn Time: 0.0033
Collected Timesteps: 51,200
Total Timesteps: 6,010,880
Total Iterations: 69




PPOLearner: Set learning rate to [2.999970e-04, 2.999970e-04]








========================================
Average Step Reward: 0.6094
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 166,764.7500
Consumption Steps/Second: 610,155.8750
Overall Steps/Second: 130,968.9688

Collection Time: 0.3193
 - Inference Time: 0.0731
 - Env Step Time: 0.1156
Consumption Time: 0.0873
 - GAE Time: 0.0632
 - PPO Learn Time: 0.0026
Collected Timesteps: 53,248
Total Timesteps: 6,064,128
Total Iterations: 70




PPOLearner: Set learning rate to [2.999970e-04, 2.999970e-04]








========================================
Average Step Reward: 0.7294
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 79,577.1953
Consumption Steps/Second: 742,351
Overall Steps/Second: 71,872.7188

Collection Time: 0.9394
 - Inference Time: 0.5295
 - Env Step Time: 0.1826
Consumption Time: 0.1007
 - GAE Time: 0.0679
 - PPO Learn Time: 0.0031
Collected Timesteps: 74,752
Total Timesteps: 6,138,880
Total Iterations: 71




PPOLearner: Set learning rate to [2.999969e-04, 2.999969e-04]








========================================
Average Step Reward: 0.7612
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 182,754.6875
Consumption Steps/Second: 526,403.3125
Overall Steps/Second: 135,657.6094

Collection Time: 0.4258
 - Inference Time: 0.1048
 - Env Step Time: 0.1471
Consumption Time: 0.1478
 - GAE Time: 0.1146
 - PPO Learn Time: 0.0033
Collected Timesteps: 77,824
Total Timesteps: 6,216,704
Total Iterations: 72




PPOLearner: Set learning rate to [2.999968e-04, 2.999968e-04]








========================================
Average Step Reward: 0.6388
Policy Entropy: 0

Policy Update Magnitude: 0
Critic Update Magnitude: 0

Collection Steps/Second: 80,594.8984
Consumption Steps/Second: 695,805.8125
Overall Steps/Second: 72,228.6797

Collection Time: 0.7623
 - Inference Time: 0.4256
 - Env Step Time: 0.1551
Consumption Time: 0.0883
 - GAE Time: 0.0623
 - PPO Learn Time: 0.0037
Collected Timesteps: 61,440
Total Timesteps: 6,278,144
Total Iterations: 73




PPOLearner: Set learning rate to [2.999968e-04, 2.999968e-04]
